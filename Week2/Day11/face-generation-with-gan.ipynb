{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nfrom tqdm import tqdm\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\n\nIMAGE_SHAPE = (128, 128, 3)\nNOISE_DIM = 100\nBATCH_SIZE = 128\nEPOCHS = 50\nDATA_PATH = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'\n\ndef preprocess_image(file_path):\n    img_raw = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img_raw, channels=3)\n    img = tf.image.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n    img = (img / 127.5) - 1.0  # Normalize to [-1, 1]\n    return img\n\ndef load_celeba_images(path):\n    all_files = [os.path.join(path, f) for f in os.listdir(path)[:10000] if f.endswith('.jpg')]\n    dataset = tf.data.Dataset.from_tensor_slices(all_files)\n    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n\n    images_list = []\n    for batch in tqdm(dataset.batch(1024).prefetch(tf.data.AUTOTUNE)):\n        images_list.append(batch)\n\n    return tf.concat(images_list, axis=0)\n\nimages = load_celeba_images(DATA_PATH)\ndataset = tf.data.Dataset.from_tensor_slices(images) \\\n                         .shuffle(buffer_size=50000) \\\n                         .batch(BATCH_SIZE) \\\n                         .prefetch(tf.data.AUTOTUNE)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:28:30.458162Z","iopub.execute_input":"2025-03-02T05:28:30.458573Z","iopub.status.idle":"2025-03-02T05:28:44.311333Z","shell.execute_reply.started":"2025-03-02T05:28:30.458534Z","shell.execute_reply":"2025-03-02T05:28:44.310162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef build_generator(NOISE_DIM=NOISE_DIM):\n    model = tf.keras.Sequential([\n        # 1) Start with a dense layer that reshapes to 8x8 with 512 channels\n        layers.Dense(8*8*512, use_bias=False, input_shape=(NOISE_DIM,)),\n        layers.Reshape((8, 8, 512)),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # 2) Upsample to 16x16\n        layers.Conv2DTranspose(256, (4,4), strides=(2,2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # 3) Upsample to 32x32\n        layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # 4) Upsample to 64x64\n        layers.Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # 5) Upsample to 128x128\n        layers.Conv2DTranspose(32, (4,4), strides=(2,2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # 6) Final layer â†’ 128x128x3, use tanh for output in [-1,1]\n        layers.Conv2DTranspose(3, (4,4), strides=(1,1), padding='same', use_bias=False, activation='tanh')\n    ], name=\"Generator\")\n\n    return model\n\n\ndef build_discriminator():\n    model = tf.keras.Sequential([\n        # 1) Downsample from 128x128 to 64x64\n        layers.Conv2D(64, (4,4), strides=(2,2), padding='same', \n                      input_shape=(128,128,3)),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Dropout(0.3),\n\n        # 2) Downsample to 32x32\n        layers.Conv2D(128, (4,4), strides=(2,2), padding='same'),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Dropout(0.3),\n\n        # 3) Downsample to 16x16\n        layers.Conv2D(256, (4,4), strides=(2,2), padding='same'),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Dropout(0.3),\n\n        # 4) Downsample to 8x8\n        layers.Conv2D(512, (4,4), strides=(2,2), padding='same'),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Dropout(0.3),\n\n        # 5) Flatten to a single logit\n        layers.Flatten(),\n        layers.Dense(1)\n    ], name=\"Discriminator\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:28:44.332098Z","iopub.execute_input":"2025-03-02T05:28:44.332317Z","iopub.status.idle":"2025-03-02T05:28:44.34201Z","shell.execute_reply.started":"2025-03-02T05:28:44.332295Z","shell.execute_reply":"2025-03-02T05:28:44.341029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = build_generator()\ndiscriminator = build_discriminator()\n\n\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    return real_loss + fake_loss\n\ngen_optimizer = tf.keras.optimizers.Adam(1e-4)\ndisc_optimizer = tf.keras.optimizers.Adam(1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:28:45.639278Z","iopub.execute_input":"2025-03-02T05:28:45.639635Z","iopub.status.idle":"2025-03-02T05:28:45.801787Z","shell.execute_reply.started":"2025-03-02T05:28:45.639606Z","shell.execute_reply":"2025-03-02T05:28:45.800788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_step(real_images):\n    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        \n        # Ensure shapes match\n        assert generated_images.shape[1:] == IMAGE_SHAPE, \"Generated image shape mismatch!\"\n        assert real_images.shape[1:] == IMAGE_SHAPE, \"Real image shape mismatch!\"\n\n        real_output = discriminator(real_images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        g_loss = generator_loss(fake_output)\n        d_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n\n    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n    return g_loss, d_loss\n\n\ndef generate_and_show_images(noise):\n    # Generate images\n    preds = generator(noise, training=False)\n    preds = (preds + 1) / 2.0  # Shift from [-1,1] to [0,1]\n    \n    # Plot\n    fig = plt.figure(figsize=(4,4))\n    for i in range(preds.shape[0]):\n        plt.subplot(4,4,i+1)\n        plt.imshow(preds[i])\n        plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:28:46.761059Z","iopub.execute_input":"2025-03-02T05:28:46.761382Z","iopub.status.idle":"2025-03-02T05:28:46.768872Z","shell.execute_reply.started":"2025-03-02T05:28:46.761356Z","shell.execute_reply":"2025-03-02T05:28:46.767721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def train(dataset, epochs):\n#     fixed_noise = tf.random.normal([16, NOISE_DIM])  # Fixed noise for generating sample images\n#     total_start_time = time.time()  # Start time for total training\n\n#     for epoch in range(epochs):\n#         epoch_start_time = time.time()  # Start time for the current epoch\n\n#         g_losses = []\n#         d_losses = []\n\n#         for image_batch in dataset:\n#             g_loss, d_loss = train_step(image_batch)\n#             g_losses.append(g_loss)\n#             d_losses.append(d_loss)\n\n#         # Calculate average losses for the epoch\n#         avg_g_loss = tf.reduce_mean(g_losses)\n#         avg_d_loss = tf.reduce_mean(d_losses)\n\n#         # Time taken for the epoch\n#         epoch_end_time = time.time()\n#         epoch_duration = epoch_end_time - epoch_start_time\n\n#         # Print epoch details\n#         print(f\"Epoch {epoch+1}/{epochs} | \"\n#               f\"Generator loss: {avg_g_loss:.4f}, \"\n#               f\"Discriminator loss: {avg_d_loss:.4f} | \"\n#               f\"Time: {epoch_duration:.2f} seconds\")\n\n#         # Generate and show images for the current epoch\n#         generate_and_show_images(fixed_noise)\n\n#     # Total time taken for training\n#     total_end_time = time.time()\n#     total_duration = total_end_time - total_start_time\n#     print(f\"\\nTotal Training Time: {total_duration:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:28:49.27809Z","iopub.execute_input":"2025-03-02T05:28:49.278456Z","iopub.status.idle":"2025-03-02T05:28:49.284763Z","shell.execute_reply.started":"2025-03-02T05:28:49.278426Z","shell.execute_reply":"2025-03-02T05:28:49.283762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nstrategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n\nwith strategy.scope():\n    generator = build_generator()\n    discriminator = build_discriminator()\n\n    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n    def generator_loss(fake_output):\n        return cross_entropy(tf.ones_like(fake_output), fake_output)\n\n    def discriminator_loss(real_output, fake_output):\n        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n        return real_loss + fake_loss\n\n    gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n    disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n\n# Existing dataset creation\ndataset = tf.data.Dataset.from_tensor_slices(images).shuffle(10000).batch(BATCH_SIZE)\n\n# Make a distributed dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n@tf.function\n\ndef distributed_train_step(dataset_inputs):\n    def step_fn(real_images):\n        return train_step(real_images)  # The train_step you already have\n\n    per_replica_g_loss, per_replica_d_loss = strategy.run(step_fn, args=(dataset_inputs,))\n    mean_g_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_g_loss, axis=None)\n    mean_d_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_d_loss, axis=None)\n    return mean_g_loss, mean_d_loss\n\n\ndef train(dataset, epochs):\n    fixed_noise = tf.random.normal([16, NOISE_DIM])\n    total_start_time = time.time()\n\n    for epoch in range(epochs):\n        epoch_start_time = time.time()\n        \n        g_losses = []\n        d_losses = []\n        \n        for image_batch in dist_dataset:  # Use distributed dataset\n            g_loss, d_loss = distributed_train_step(image_batch)\n            g_losses.append(g_loss)\n            d_losses.append(d_loss)\n        \n        avg_g_loss = tf.reduce_mean(g_losses)\n        avg_d_loss = tf.reduce_mean(d_losses)\n        \n        epoch_duration = time.time() - epoch_start_time\n        print(f\"Epoch {epoch+1}/{epochs} | \"\n              f\"Generator loss: {avg_g_loss:.4f}, \"\n              f\"Discriminator loss: {avg_d_loss:.4f} | \"\n              f\"Time: {epoch_duration:.2f} s\")\n        \n        generate_and_show_images(fixed_noise)\n\n    total_duration = time.time() - total_start_time\n    print(f\"\\nTotal Training Time: {total_duration:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:28:51.006957Z","iopub.execute_input":"2025-03-02T05:28:51.007274Z","iopub.status.idle":"2025-03-02T05:28:53.189052Z","shell.execute_reply.started":"2025-03-02T05:28:51.007252Z","shell.execute_reply":"2025-03-02T05:28:53.18821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(dataset, 100)\ngenerator.save('generator_800.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:51:41.90116Z","iopub.execute_input":"2025-03-02T16:51:41.90151Z","execution_failed":"2025-03-02T16:59:23.661Z"}},"outputs":[],"execution_count":null}]}